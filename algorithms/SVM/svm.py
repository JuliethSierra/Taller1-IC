# -*- coding: utf-8 -*-
"""last_svm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aPFH4z43oK9dWlocY05kajSUPCkn_Tmy

# Máquina de soporte de vectores
"""

# Librerias base

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sb

#Preprocesamiento y modelado
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import KFold

import warnings
warnings.filterwarnings('ignore')

"""# Carga de datos y preprocesamiento"""

# Carga de datos
url = 'https://www.datos.gov.co/resource/ynam-yc42.csv' #Cambiar ruta a CSV con valores normalizados
# url = '/content/ynam-yc42.csv' # Tabla de valores sin normalizar
dataset = pd.read_csv(url)

# Dimensiones y registros del dataset
print(dataset.shape)
# Primeros registros
dataset.head()

#Verificar el top 10 de variables con faltantes, ninguno supera el 10%, por lo que
#no seria muy dañino descartar los registros faltantes para simplificar el analisis.

def describe_nan(df):
    return pd.DataFrame([(i, df[df[i].isna()].shape[0], df[df[i].isna()].shape[0]/df.shape[0]*100) for i in df.columns], columns=['Variable', 'NAN_Conteo', 'NAN_%'])


describe_nan(dataset).sort_values(by='NAN_%',ascending=False)

"""Para el presente trabajo, nuestra variable de interés es 'estu_generacion_e' que representa si el estudiante forma parte del programa Generación E que le apunta a la transformación social y al desarrollo de las regiones del país a través del acceso, permanencia y graduación a la educación superior de los jóvenes en condición de vulnerabilidad económica.

Fuente: https://especiales.colombiaaprende.edu.co/generacione/programa.html
"""

# La variable estu_generacion_e es de tipo objeto; por tanto se creara una nueva
# variable mediante un proceso de encoding que representara la variable original
# y facilitara su analisis y modelamiento posterior.

dataset['estu_generacion_e'].unique()

# Usando label_encoder codificamos para normalizar o dummificar la variable.
encoder = preprocessing.LabelEncoder()
dataset['generacion_e']= encoder.fit_transform(dataset['estu_generacion_e'])
dataset['generacion_e'].unique()

unique_categories = encoder.classes_

label_to_category_mapping = {label: category for label,
                             category in enumerate(unique_categories)}

for label, category in label_to_category_mapping.items():
    print(f"La etiqueta {label} representa la categoria '{category}'")

"""**Selección de variables**

El programa Generacion E esta enfocado en jovenes de situacion economica vulnerable ademas de aquellos con buen desempeño académico, por tanto las variables de interés para el presente analisis se seleccionaran de acuerdo a su pertinencia y aporte a este tipo de información.
"""

# Variables seleccionadas

datasetv1 = dataset[['estu_genero',
                     'estu_tieneetnia',
                     'fami_estratovivienda',
                     'fami_cuartoshogar',
                     'fami_educacionpadre',
                     'fami_educacionmadre',
                     'fami_trabajolaborpadre',
                     'fami_trabajolabormadre',
                     'fami_tieneinternet',
                     'fami_tieneserviciotv',
                     'fami_tienecomputador',
                     'fami_tienelavadora',
                     'fami_tienehornomicroogas',
                     'fami_tieneautomovil',
                     'fami_tienemotocicleta',
                     'fami_tieneconsolavideojuegos',
                     'fami_situacioneconomica',
                     'cole_naturaleza',
                     'desemp_ingles',
                     'generacion_e',
                     'punt_global',
                     'percentil_global']]

# Obtención de listado de nombre de las columnas
var_numericas = datasetv1.select_dtypes(include=np.number).columns.to_list()
var_categoricas = datasetv1.select_dtypes(include=np.object).columns.to_list()

# En muchos registros el dato se representa con '-' el cual se interpretó como
# desconocimiento del entrevistado o hay una carencia de datos.
datasetv2 = datasetv1.replace('-',np.nan)

# Revisión de las categorías / opciones disponibles por variable

for variable in var_categoricas:
    valores_unicos = datasetv2[variable].unique()
    print(f"Variable: {variable}")
    print("Categorias:")
    for value in valores_unicos:
        print(value)
    print("\n")

# Ordenamiento de las variables con mayor # de datos faltantes de mayor a menor.
describe_nan(datasetv2).sort_values(by='NAN_%',ascending=False)

"""**Tratamiento de registros faltantes**

Para aquellas variables cuyo porcentaje de valores faltantes son mayores a 5% se intentó imputar el valor faltante con alguna de las categorias posibles dentro de la variable. Para aquellas que presenten un porcentaje de valores faltantes menor a 5% se descartaran los registros.
"""

# Reemplazo de los valores faltantes con la categoria 'Sin Estrato'
datasetv2['fami_estratovivienda'].fillna('Sin Estrato',inplace=True)

# Reemplazo de los valores faltantes con la categoria 'No sabe'
datasetv2['fami_educacionpadre'].fillna('No sabe',inplace=True)

# Reemplazo de los valores faltantes con la categoria 'No sabe'
datasetv2['fami_educacionmadre'].fillna('No sabe',inplace=True)

# Reemplazo de los valores faltantes con la categoria 'No sabe'
datasetv2['fami_trabajolabormadre'].fillna('No sabe',inplace=True)

# Reemplazo de los valores faltantes con la categoria 'No sabe'
datasetv2['fami_trabajolaborpadre'].fillna('No sabe',inplace=True)

# Lo ideal es revisar una a una las variables y determinar si se puede imputar
# el valor faltante, o si definitivamente se descarta, para este ejercicio
# se van a descartar los demas registros que contengan datos faltantes.

datasetv3 = datasetv2.dropna()

print(f"El dataset final tiene la siguiente forma:\n"
      f"- Numero de registros: {datasetv3.shape[0]}\n"
      f"- Numero de variables: {datasetv3.shape[1]}")

# Verificación de opciones / categorías por variable
for variable in var_categoricas:
    valores_unicos = datasetv3[variable].unique()
    print(f"Variable: {variable}")
    print("Categorias:")
    for value in valores_unicos:
        print(value)
    print("\n")

# Asignación de clase categorica a las variables
datasetv3[var_categoricas] = datasetv3[var_categoricas].astype('category')

# Separación del dataset entre variables independientes X y variable de respuesta 'y'
X, y = datasetv3.drop(columns='generacion_e'), datasetv3['generacion_e']

# Obtención de la lista de variables a utilizar
var_numericas = X.select_dtypes(include=np.number).columns.to_list()
var_categoricas = X.select_dtypes(include='category').columns.tolist()

# Separación del dataset de entrenamiento y de prueba
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.3,
                                                    random_state=11)

# Creación de un pipeline de procesamiento (aplicar unos pasos definidos para
# no rehacer el procesamiento con cada cambio o necesidad, los pasos
# normalizan las variables numericas usando un scala estandar y onehotencoder
# para las variables categoricas
numeric_transformer = Pipeline(
    steps=[("scaler", StandardScaler())]
)

categorical_transformer = OneHotEncoder(handle_unknown="ignore", drop="if_binary")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, var_numericas),
        ("cat", categorical_transformer, var_categoricas),
    ]
)

# Uso de modelo lasso como herramienta de 'selección de características',
# es decir, elegir que variables usar para entrenar el modelo de acuerdo a su
# significancia. En este caso no tiene mucho sentido puesto que ya se habían
# seleccionado las variables que iban a utilizar. Lo ideal seria aplicar el
# lasso al dataset completo pero teniendo en cuenta que debe limpiarse de
# registros nulos y demas.

from sklearn.linear_model import Lasso
pipeline = Pipeline([('data_processor',preprocessor),('lasso', Lasso())])
search = GridSearchCV(pipeline,
                      {'lasso__alpha': np.arange(0.01, 10, 1)},
                      cv=5, scoring="neg_mean_squared_error", verbose=3
                      )
search.fit(X_train, y_train)

# Mejor alpha seleccionado
search.best_params_

coefficients = search.best_estimator_.named_steps['lasso'].coef_
importance = np.abs(coefficients)
np.abs(coefficients)

# Coeficientes lasso indican aquellas variables que aun siendo afectadas por el
# alpha lasso, aquellas con coeficiente de 0 mantienen un nivel de significancia
# o efecto sobre la variable respuesta.
coeficientes_lasso = sorted(list(
    zip(search.best_estimator_.named_steps['lasso'].coef_, X_train)), reverse=True)
coeficientes_lasso

# Para este caso, que el estudiante pertenezca a una etnia, que tenga un
# automóvil y la educación del padre tienen peso sobre la variable estudiante_e

# Definición de nuevos sets de entrenamiento, solo considerando variables
# seleccionadas por lasso.
X_train_lasso = X_train[['estu_tieneetnia',
                         'fami_tieneautomovil',
                         'fami_educacionpadre']]
X_test_lasso = X_test[['estu_tieneetnia',
                       'fami_tieneautomovil',
                       'fami_educacionpadre']]

# Modificación del pipeline de procesamiento para solo considerar variables lasso.
var_categoricas_lasso = ['estu_tieneetnia',
                         'fami_tieneautomovil',
                         'fami_educacionpadre']

categorical_transformer = OneHotEncoder(handle_unknown="ignore", drop="if_binary")

preprocessor_lasso = ColumnTransformer(
    transformers=[
        ("cat", categorical_transformer, var_categoricas_lasso),
    ])

# GridSearch del modelo SVM con kernel lineal y con las variables originales
svm_classifier = SVC(probability=True)

param_grid = {
    'SVM_linear__C': [0.001, 0.01, 0.1, 1, 10],
    'SVM_linear__kernel': ['linear']
}

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('SVM_linear', svm_classifier)
])

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=5)
grid_search.fit(X_train, y_train)


best_params = grid_search.best_params_
best_score = grid_search.best_score_

results = grid_search.cv_results_
accuracy_scores = np.array(results['mean_test_score']).reshape(len(param_grid['SVM_linear__C']))

plt.figure(figsize=(8, 6))
plt.plot(param_grid['SVM_linear__C'], accuracy_scores, marker='o')
plt.title('Precisión vs C de SVM Lineal')
plt.xlabel('C')
plt.ylabel('Precisión')
plt.xscale('log')
plt.grid(True)
plt.show()

print("Mejores Hyperparametros:", best_params)
print("Mejor Puntaje de Precisión:", best_score)

best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print("Puntaje de prueba:", test_score)

# GridSearch del modelo SVM con kernel lineal y con las variables seleccionadas por lasso

svm_classifier = SVC(probability=True)

param_grid = {
    'SVM_linear__C': [0.001, 0.01, 0.1, 1, 10],
    'SVM_linear__kernel': ['linear']
}

pipeline = Pipeline([
    ('preprocessor', preprocessor_lasso),
    ('SVM_linear', svm_classifier)
])

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=5)
grid_search.fit(X_train_lasso, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

results = grid_search.cv_results_
accuracy_scores = np.array(results['mean_test_score']).reshape(len(param_grid['SVM_linear__C']))

plt.figure(figsize=(8, 6))
plt.plot(param_grid['SVM_linear__C'], accuracy_scores, marker='o')
plt.title('Precisión vs C de SVM Lineal (LASSO)')
plt.xlabel('C')
plt.ylabel('Precisión')
plt.xscale('log')
plt.grid(True)
plt.show()

print("Mejores Hyperparametros:", best_params)
print("Mejor Puntaje de Precisión:", best_score)

best_model = grid_search.best_estimator_
test_score = best_model.score(X_test_lasso, y_test)
print("Puntaje de prueba:", test_score)

# GridSearch del modelo SVM con kernel rbf y con las variables originales

svm_classifier = SVC(kernel='rbf', probability=True)

param_grid = {
    'SVM_rbf__C': [0.01, 0.1, 1, 10],
    'SVM_rbf__gamma': [0.01, 0.1, 1, 10]
}

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('SVM_rbf', svm_classifier)
])

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=5)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

results = grid_search.cv_results_
accuracy_scores = np.array(results['mean_test_score']).reshape(len(param_grid['SVM_rbf__C']), len(param_grid['SVM_rbf__gamma']))

plt.figure(figsize=(10, 8))
for idx, gamma in enumerate(param_grid['SVM_rbf__gamma']):
    plt.plot(param_grid['SVM_rbf__C'], accuracy_scores[:, idx], marker='o', label=f'gamma={gamma}')

plt.title('Precisión vs C de SVM por RBF')
plt.xlabel('C')
plt.ylabel('Precisión')
plt.xscale('log')
plt.grid(True)
plt.legend(title='Gamma')
plt.show()

print("Mejores Hyperparametros:", best_params)
print("Mejor Puntaje de Precisión:", best_score)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print("Puntaje de precisión de prueba:", test_accuracy)

# GridSearch del modelo SVM con kernel rbf y con las variables seleccionadas por lasso

svm_classifier = SVC(kernel='rbf', probability=True)

param_grid = {
    'SVM_rbf__C': [0.01, 0.1, 1, 10],
    'SVM_rbf__gamma': [0.01, 0.1, 1, 10]
}

pipeline = Pipeline([
    ('preprocessor', preprocessor_lasso),
    ('SVM_rbf', svm_classifier)
])

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=30)
grid_search.fit(X_train_lasso, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

results = grid_search.cv_results_
accuracy_scores = np.array(results['mean_test_score']).reshape(len(param_grid['SVM_rbf__C']), len(param_grid['SVM_rbf__gamma']))

plt.figure(figsize=(10, 8))
for idx, gamma in enumerate(param_grid['SVM_rbf__gamma']):
    plt.plot(param_grid['SVM_rbf__C'], accuracy_scores[:, idx], marker='o', label=f'gamma={gamma}')

plt.title('Precisión vs C de SVM por RBF (LASSO)')
plt.xlabel('C')
plt.ylabel('Precisión')
plt.xscale('log')
plt.grid(True)
plt.legend(title='Gamma')
plt.show()

print("Mejores Hyperparametros:", best_params)
print("Mejor Puntaje de Precisión:", best_score)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_lasso)
test_accuracy = accuracy_score(y_test, y_pred)
print("Puntaje de Precisión de Prueba:", test_accuracy)

"""**Consideraciones**

El dataset se encuentra desbalanceado, para la variable respuesta generacion_e hay muchos mas registros de una categoria que de las otras, seria bueno considerar: o descartar la categoria por excelencia la cual solo tiene 11 registros y usar oversampling para la segunda mayor categoria.
"""